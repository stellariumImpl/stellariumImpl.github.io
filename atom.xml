<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stellarium</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-07-17T15:10:47.505Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Felix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>正负采样</title>
    <link href="http://example.com/2025/07/18/%E6%AD%A3%E8%B4%9F%E9%87%87%E6%A0%B7/"/>
    <id>http://example.com/2025/07/18/%E6%AD%A3%E8%B4%9F%E9%87%87%E6%A0%B7/</id>
    <published>2025-07-17T15:05:02.000Z</published>
    <updated>2025-07-17T15:10:47.505Z</updated>
    
    <content type="html"><![CDATA[<h1 id="正负样本"><a href="#正负样本" class="headerlink" title="正负样本"></a>正负样本</h1><h2 id="正样本"><a href="#正样本" class="headerlink" title="正样本"></a>正样本</h2><p>正样本：曝光而且有点击的用户-物品二元组（用户对物品感兴趣）<br>问题：少部分物品占据大部分点击，导致正样本大多是热门物品<br>解决方案：过采样冷门物品，或降采样热门物品</p><ul><li>过采样 up-sampling 让一个样本出现多次</li><li>降采样 down-sampling 让一些样本被抛弃<br>正样本不需要抽样<br>正样本是用户真实点击的行为，本身就有监督标签，不需要采样。</li></ul><h2 id="负样本"><a href="#负样本" class="headerlink" title="负样本"></a>负样本</h2><p><strong>如何选取负样本？</strong></p><h3 id="简单负样本：全体物品"><a href="#简单负样本：全体物品" class="headerlink" title="简单负样本：全体物品"></a>简单负样本：全体物品</h3><p>未被召回的物品，大概率是用户不感兴趣的</p><p>未被召回的物品 几乎可以等于 全体物品</p><p>所以其实就可以从全体物品中做抽样，作为负样本</p><p>问题就在于 均匀抽样 还是 非均匀抽样？</p><p><strong>均匀抽样</strong></p><p>对冷门物品不公平</p><p>正样本 大多数是热门物品，也就是小部分热门物品占据大部分点击的</p><p>如果做均匀抽样产生负样本，会导致负样本大多数都是冷门物品</p><p><strong>非均匀抽样</strong></p><p>目的是打压热门物品</p><p>负样本抽样概率与热门程度（点击次数）正相关</p><p>抽样概率 正向关于 点击次数的多少次放，一般取值为0.75，如果是0，那就是完全均匀抽样，冷门物品占比过高，如果是1则完全按点击次数，热门物品非常容易被采到，极度冷门的物品几乎不可能被抽到，极端情况下，正负样本分布高度相似，模型难以学到判别边界，太偏向热门</p><h3 id="简单负样本：Batch内负样本"><a href="#简单负样本：Batch内负样本" class="headerlink" title="简单负样本：Batch内负样本"></a>简单负样本：Batch内负样本</h3><p>在推荐系统中，常用的一种负样本采样方法是 Batch 内负采样（In-batch Negative Sampling）。也就是说，每个 batch 中有 $n$ 个正样本，每个正样本对应的用户会把其他 $n - 1$ 个物品视为负样本，因此一个 batch 会构造出 $n \times (n - 1)$ 个负样本。这种方式非常高效，训练速度快，而且不需要额外采负样本。</p><p>但它有一个很大的问题：因为正样本来自用户点击日志，所以热门物品更容易出现在 batch 中。也就是说，一个物品成为负样本的概率实际上和它的点击次数成正比，即</p><p>$$<br>P(i) \propto (\text{点击次数}_i)^1<br>$$</p><p>但我们理想中希望负样本的采样概率不要太偏向热门物品，否则模型会过度打压这些热门物品。更合理的采样方式应是点击次数的 0.75 次方，也就是</p><p>$$<br>P(i) \propto (\text{点击次数}_i)^{0.75}<br>$$</p><p>这样可以在保持一定“热门倾向”的同时，避免热门物品出现频率过高，从而在训练时被“误判”为大量用户都不喜欢。遗憾的是，Batch 内负采样天然就是按点击次数的一次方来取负样本的，这和我们的理想采样分布是有偏差的。</p><p>为了解决这个问题，我们可以在 loss 函数中做一个重要性修正。具体来说，对于每一个负样本对 $(a, b_i)$，其中 $a$ 是用户，$b_i$ 是某个物品，我们预估用户对它的兴趣分为 $\cos(a, b_i)$，同时我们知道该物品出现在 batch 中作为负样本的概率为 $p_i$，那么我们可以调整这个负样本的打分为：</p><p>$$<br>\cos(a, b_i) - \log p_i<br>$$</p><p>这个 $-\log p_i$ 项的作用就是“削弱”热门物品在负样本中出现太多带来的打压。也就是说，如果某个物品是热门的，那么它的 $p_i$ 比较大，$\log p_i$ 就也比较大，整体负样本打分会被“往上调”，避免模型惩罚得太狠。</p><p>比如某个热门物品 A 的点击次数多，导致 $p_A &#x3D; 0.05$，而 $\cos(a, b_A) &#x3D; 0.3$，那么：</p><p>$$<br>\text{Adjusted score} &#x3D; 0.3 - \log(0.05) \approx 0.3 + 3.0 &#x3D; 3.3<br>$$</p><p>可以看到，这个调整大大降低了热门物品作为负样本时的惩罚力度，让模型更“公平”地学习它的特征。</p><p>这个思想本质上是 Noise Contrastive Estimation（NCE）的一个具体体现：我们知道我们的负样本分布与目标分布不一致，那么就用 $-\log(\text{noise prob})$ 来进行纠偏。</p><p>总的来说，Batch 内负采样虽然高效，但存在对热门物品的惩罚偏高问题。通过在训练时减去 $\log p_i$，我们可以有效地实现重要性加权，让负样本分布和我们理想的 $ (\text{点击次数})^{0.75} $ 分布更加一致，从而提升推荐模型的表现，线上召回的时候不用减，还是用原来的余弦相似度</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;正负样本&quot;&gt;&lt;a href=&quot;#正负样本&quot; class=&quot;headerlink&quot; title=&quot;正负样本&quot;&gt;&lt;/a&gt;正负样本&lt;/h1&gt;&lt;h2 id=&quot;正样本&quot;&gt;&lt;a href=&quot;#正样本&quot; class=&quot;headerlink&quot; title=&quot;正样本&quot;&gt;&lt;/a&gt;正样</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="负样本" scheme="http://example.com/tags/%E8%B4%9F%E6%A0%B7%E6%9C%AC/"/>
    
    <category term="热门物品打压" scheme="http://example.com/tags/%E7%83%AD%E9%97%A8%E7%89%A9%E5%93%81%E6%89%93%E5%8E%8B/"/>
    
  </entry>
  
  <entry>
    <title>双塔模型</title>
    <link href="http://example.com/2025/07/05/%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2025/07/05/%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B/</id>
    <published>2025-07-05T11:32:52.000Z</published>
    <updated>2025-07-17T15:04:47.440Z</updated>
    
    <content type="html"><![CDATA[<h3 id="离散特征"><a href="#离散特征" class="headerlink" title="离散特征"></a>离散特征</h3><p><strong>何为离散特征？</strong><br>性别：男、女<br>国籍：中国、美国…<br>英文单词：常见的英文单词有几万个<br>物品ID：小红书有几亿篇笔记，每篇笔记都有一个ID<br>用户ID：小红书有几亿个用户，每个用户有一个ID</p><p><strong>离散特征的处理</strong></p><ol><li>建立字典：把类别映射成序号</li></ol><ul><li>中国-&gt;1</li><li>美国-&gt;2</li><li>澳洲-&gt;3</li></ul><ol start="2"><li>向量化：把序号映射成向量</li></ol><ul><li>One-hot编码：把序号映射成高维稀疏向量</li><li>Embedding：把序号映射成低维稠密向量</li></ul><p><strong>One-hot是怎么做的</strong><br>比如用200维稀疏向量表示国籍</p><ul><li>未知-&gt;0-&gt;[0,0,0,…,0]</li><li>中国-&gt;1-&gt;[1,0,0,…,0]</li><li>美国-&gt;2-&gt;[0,1,0,…,0]</li></ul><p><strong>One-hot的局限</strong><br>对于性别这样的特征用One-hot还算可以，但是对于很多其他场景就不合适了，自然语言处理中，对单词做编码，英文有几万个单词，那么One-hot向量的维度是几万；推荐系统中，对物品ID做编码，小红书有几亿篇笔记，那么One-hot向量的维度是几亿。</p><p><strong>Embedding是怎么做的</strong><br><strong>参数数量</strong>：向量维度*类别数量</p><ul><li>设Embedding得到的向量都是4维的</li><li>一共有200个国籍</li><li>参数数量&#x3D;4*200&#x3D;800</li></ul><p><strong>编程实现</strong>：TensorFlow、Pytorch提供的embedding层</p><ul><li>参数以矩阵形式保存，矩阵大小也就是参数数量</li><li>输入是序号，比如美国的序号是2</li><li>输出是向量，比如美国对应参数矩阵的第2列</li></ul><p>Embedding其实就是<strong>参数矩阵</strong>乘One-hot向量<br><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/image.png" alt="Embedding"></p><h3 id="矩阵补充"><a href="#矩阵补充" class="headerlink" title="矩阵补充"></a>矩阵补充</h3><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/%E7%9F%A9%E9%98%B5%E8%A1%A5%E5%85%85.png" alt="矩阵补充"><br>上图a和b分别是用户ID和物品ID的Embedding层输出的某一列，每一列代表一个用户或者物品，两个Embedding层不共享参数，a和b向量做内积得到a用户对b物品预估的偏好得分，<strong>模型训练</strong>的目的就是学习矩阵A和B，使得预估值拟合真是观测的兴趣分数</p><ul><li><p>把用户 ID、物品 ID 映射成向量：</p><ul><li>第 $u$ 号用户 $\rightarrow$ 向量 $\mathbf{a}_u$</li><li>第 $i$ 号物品 $\rightarrow$ 向量 $\mathbf{b}_i$</li></ul></li><li><p>求解优化问题，得到参数 $\mathbf{A}$ 和 $\mathbf{B}$：</p></li></ul><p>$$<br>\min_{\mathbf{A}, \mathbf{B}} \sum_{(u, i, y) \in \Omega} \left( y - \langle \mathbf{a}_u, \mathbf{b}_i \rangle \right)^2<br>$$</p><p>其中：</p><ul><li>$\Omega$ 表示训练集中已观察的评分集合；</li><li>$\langle \mathbf{a}_u, \mathbf{b}_i \rangle$ 表示用户与物品向量的内积；</li><li>$y$ 是用户 $u$ 对物品 $i$ 的真实评分。</li></ul><p><strong>矩阵补充在实践中效果不好</strong></p><ul><li>缺点1:仅用ID embedding，没有利用物品、用户属性</li><li>缺点2:负样本的选取方式不对，样本为用户-物品二元组，记作(u,i)，正样本的选择是正确的，曝光之后，有点击、交互就算正样本；但是负样本的选择为曝光之后，没有点击、交互，这是错误的，因为虽然有一部分是系统认为用户喜欢的，但用户确实不喜欢，这一部分是真正的负样本，但还有一部分可能是用户可能感兴趣但没看到，这些不是真正的负样本，把它们强行标记为负样本，会误导模型</li><li>缺点3：做训练的方法不好，内积不如余弦相似度；并且用平方损失（回归）也不如用交叉熵损失（分类）</li></ul><p><strong>模型存储</strong></p><ul><li>训练得到矩阵A和B，A的每一列对应一个用户，B的每一列对应一个物品</li><li>对于矩阵A，可以把A的列存储到key-value表，key是用户ID，value是A的一列</li></ul><p><strong>线上服务</strong><br>把用户ID作为key，查询key-value表，得到该用户的向量，记作a<br>最近邻查找：查找用户最有可能感兴趣的k个物品，作为召回结果</p><ul><li>第i号物品的embedding向量记作$b_i$</li><li>内积$&lt;a,b_i&gt;$是用户对第i号物品兴趣的预估</li><li>返回内积最大的k个物品<br>如果枚举所有物品，时间复杂度正比于物品数量</li></ul><p><strong>如何加速最近邻查找</strong></p><p>近似最近邻查找 Approximate Nearest Neighbor Search</p><p>支持最近邻查找的系统：Milvus、Faiss、HnswLib</p><p>衡量最近邻的标准：</p><ul><li>欧式距离最小（L2距离）</li><li>向量内积最大（内积相似度）</li><li>向量夹角余弦最大（cosine相似度）</li></ul><p><strong>加速最近邻查找的方法</strong></p><p>划分扇形图，按照最近邻衡量标准不同的划分方法，划分为n个区域，每个区域用一个向量代表这个区域，将n个代表向量为key，每个区域中的点为value。</p><p>这样就可以快速做线上召回了，只用将某个用户embedding和每个物品代表向量做相似度计算，以找到最相似的代表向量，也就找到了里面的value，再对这个区域里所有的点做相似度计算，大大减小了计算次数</p><p><strong>线上召回</strong></p><p>把用户向量a作为query，查找使得$&lt;a,b_i&gt;$最大化的物品i</p><p>暴力枚举速度太慢，实践中用近似最近邻查找</p><p>工业界会用一些开源的向量数据库Milvus、Faiss、HnswLib，支持最近邻查找</p><h3 id="双塔模型"><a href="#双塔模型" class="headerlink" title="双塔模型"></a>双塔模型</h3><p>矩阵补充模型的升级版，矩阵补充只用到了用户和物品的ID，比较弱，没有用到两者分别的属性</p><p>下图为得到用户表征向量和物品表征向量就可以做余弦相似度计算相似度score</p><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/alldualtower.png" alt="双塔模型总览"></p><h4 id="用户属性"><a href="#用户属性" class="headerlink" title="用户属性"></a>用户属性</h4><p>用户ID，同矩阵补充模型，使用一个Embedding Layer</p><p>用户离散特征，每个特征使用一个Embedding Layer，有n个离散特征（如果特征值比较少，比如性别这样的，可以用one-hot）</p><p>用户连续特征，归一化、分桶等处理</p><p>拼接起来输送到神经网络，输出用户的表征</p><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/userattribute.png" alt="用户属性embedding"></p><h4 id="物品属性"><a href="#物品属性" class="headerlink" title="物品属性"></a>物品属性</h4><p>同用户属性，得到物品的表征</p><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/itemattribute.png" alt="用户属性embedding"></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul><li>Pointwise：独立看待每个正样本、负样本，做简单的二元分类</li><li>Pairwise：每次取一个正样本、一个负样本</li><li>Listwise：每次取一个正样本、多个负样本</li></ul><p><strong>正负样本的选择</strong></p><p>正样本：用户点击的物品<br>负样本：</p><ul><li>没有被召回的？</li><li>召回但是被粗排、精排淘汰的？</li><li>曝光但是没有被点击的？</li></ul><h4 id="Pointwise"><a href="#Pointwise" class="headerlink" title="Pointwise"></a>Pointwise</h4><p>好处：召回阶段的目标是判断用户-物品对的“相关性”（如点击&#x2F;未点击），本质是二分类问题（正样本&#x3D;用户感兴趣，负样本&#x3D;不感兴趣）。</p><p>对于正样本，鼓励$cos(a,b)$接近1，<br>对于负样本，鼓励$cos(a,b)$接近-1。</p><p>控制正负样本的数量为1:2或者1:3，为什么？</p><p><strong>解决数据不平衡问题</strong></p><p>真实场景中，正样本（点击&#x2F;交互）远少于负样本（比如说未点击），若负样本过多，模型会偏向预测为负（准确率虽高但召回率低），1:2 或 1:3 的比例 是对负样本的降采样，平衡类别分布，避免模型偏见。</p><p>Pointwise 简单高效，适合大规模召回</p><p>在召回阶段，快速筛选候选集比精确排序更重要，因此 Pointwise 更常用。</p><h4 id="Pairwise"><a href="#Pairwise" class="headerlink" title="Pairwise"></a>Pairwise</h4><p>物品不管正负样本，神经网络中的参数共享</p><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/pariwise.png" alt="pairwise"></p><p>鼓励$cos(a,b^+)$大于$cos(a,b^-)$</p><ul><li>如果$cos(a,b^+)$大于$cos(a,b^-)+m$，则没有损失，其中m是一个超参数</li><li>否则，损失等于$cos(a,b^-)+m-cos(a,b^+)$，那就说明有损失</li></ul><p>可以推出Triplet hinge loss</p><p>$$<br>L(a,b^+,b^-)&#x3D;max\{0,cos(a,b^-)+m-cos(a,b^+)\}<br>$$</p><p>Triplet logistic loss</p><p>$$<br>L(a,b^+,b^-)&#x3D;log(1+exp[\sigma.(cos(a,b^-)-cos(a,b^+))])<br>$$</p><h4 id="Listwise"><a href="#Listwise" class="headerlink" title="Listwise"></a>Listwise</h4><p>一条数据包含：</p><ul><li>一个用户，特征向量记作a</li><li>一个正样本，特征向量记作$b^+$</li><li>多个负样本，特征向量记作$b_1^-,…,b_n^-$</li></ul><p>鼓励$cos(a,b^+)$尽量大<br>鼓励$cos(a,b_1^-),…,cos(a,b_n^-)$尽量小</p><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/listwise.png" alt="listwise"></p><h3 id="前期融合or后期融合"><a href="#前期融合or后期融合" class="headerlink" title="前期融合or后期融合"></a>前期融合or后期融合</h3><p><img src="https://gcore.jsdelivr.net/gh/stellariumImpl/CDN/pic/qqrh.png" alt="前期融合"></p><p>该图展示的是一种前期融合模型，将用户和物品的特征（如 ID、离散特征、连续特征）分别处理后concatenate输入到一个神经网络中预测用户对物品的兴趣。</p><p>这种结构常用于 CTR 预估，但并不适用于召回阶段。</p><p><strong>双塔模型适合召回阶段的原因</strong></p><p>双塔模型将用户和物品的特征分别通过神经网络编码为独立向量（Embedding），分别记作 u 和 v，用点积或余弦相似度计算两者之间的匹配度 <code>score = dot(u, v)</code>。</p><p>优点在于可以提前离线计算物品向量，只需在线计算用户向量后检索相似物品（如用 Faiss）。</p><p>适合大规模召回，在大规模候选集中（百万级、亿级物品），提前建好物品向量索引，用户进来后快速检索相似物品，效率高。</p><p><strong>后期融合？</strong></p><p>后期融合指的是：用户向量、物品向量先各自独立学习和生成，最后在推理阶段进行后期的相似度计算或打分。</p><p>在训练阶段可以使用点积 + Sigmoid 或 Softmax loss，优化匹配概率。</p><p>后期融合的特点是：结构上是解耦的，可扩展的，可缓存的，完全适合召回。</p><p><strong>前期融合不适用于召回阶段的原因</strong></p><ul><li><p>用户和物品强耦合，无法分开计算，因为模型结构上将用户特征和物品特征拼接在一起输入神经网络，无法事先计算用户或物品的独立向量，每次都要组合后重新计算——不能提前离线建立索引。</p></li><li><p>无法实现向量检索，输出不是标准向量对之间的相似度，而是神经网络回归结果，缺乏对称性与嵌入空间的结构化表达，不适用于 Faiss 等库</p></li><li><p>设计本质是“打分”不是“召回”，适用于排序阶段的小规模候选集打分，不适合百亿级候选池的粗筛。</p></li></ul><p><strong>使用建议</strong></p><ul><li><p>双塔模型用于召回（粗排）；</p></li><li><p>前期融合或深度交叉模型（如DIN、DeepFM）用于排序阶段。这样才能兼顾召回效率与排序精度。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;离散特征&quot;&gt;&lt;a href=&quot;#离散特征&quot; class=&quot;headerlink&quot; title=&quot;离散特征&quot;&gt;&lt;/a&gt;离散特征&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;何为离散特征？&lt;/strong&gt;&lt;br&gt;性别：男、女&lt;br&gt;国籍：中国、美国…&lt;br&gt;英文单词：常见的英文单</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="离散特征处理" scheme="http://example.com/tags/%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86/"/>
    
    <category term="矩阵补充" scheme="http://example.com/tags/%E7%9F%A9%E9%98%B5%E8%A1%A5%E5%85%85/"/>
    
    <category term="双塔模型" scheme="http://example.com/tags/%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Swing</title>
    <link href="http://example.com/2025/07/05/Swing/"/>
    <id>http://example.com/2025/07/05/Swing/</id>
    <published>2025-07-05T08:36:57.000Z</published>
    <updated>2025-07-05T11:34:08.119Z</updated>
    
    <content type="html"><![CDATA[<h3 id="传统ItemCF的局限性"><a href="#传统ItemCF的局限性" class="headerlink" title="传统ItemCF的局限性"></a>传统ItemCF的局限性</h3><p><strong>热门物品偏差</strong>：热门物品容易频繁共现，导致相似度虚高，比如书店里《哈利波特》和《新华字典》被大量客户购买，但二者并无实际关联</p><p><strong>稀疏性问题</strong>：用户-物品交互数据稀疏时，传统相似度（如余弦）虽然可以用，但是计算不稳定。</p><h3 id="Swing-的改进思路"><a href="#Swing-的改进思路" class="headerlink" title="Swing 的改进思路"></a>Swing 的改进思路</h3><p>如果两个物品被同一批用户交互过，则它们可能相似</p><p>进一步考虑共现用户的重叠程度，避免热门物品的干扰。</p><p>相似度计算公式为</p><p>$$<br>\text{Sim}(i, j) &#x3D; \sum_{u \in U_i \cap U_j} \sum_{v \in U_i \cap U_j} \frac{1}{\alpha + |I_u \cap I_v|}<br>$$</p><p>其中：</p><ul><li>$U_i$：交互过物品 $i$ 的用户集合  </li><li>$I_u$：用户 $u$ 交互过的物品集合  </li><li>$\alpha$：平滑参数（通常取1，防止分母为 0）</li></ul><p>这么计算的原因是如果两个用户u和v都交互了物品i和j，且它们共同交互的其他物品很少（即$|I_u \cap I_v|$小），则i和j的相似度更高</p><p>这样可以降低热门物品的权重，因为热门物品通常会被很多用户共同交互。</p><h3 id="Swing的算法流程"><a href="#Swing的算法流程" class="headerlink" title="Swing的算法流程"></a>Swing的算法流程</h3><p><strong>步骤1：构建用户-物品共现图</strong></p><ul><li>对于每个物品i，记录交互过它的用户集合$U_i$</li><li>对于每对物品<code>(i,j)</code>，找到共同交互的用户$U_i \cap U_j$</li></ul><p><strong>步骤2：计算物品相似度</strong></p><ul><li>对于每对共现用户<code>(u,v)</code>，计算他们共同交互物品数$|I_u \cap I_v|$</li><li>使用Swing公式计算相似度</li></ul><p><strong>步骤3：归一化</strong></p><p>对相似度矩阵进行归一化（如最大值归一化）。</p><p><strong>为什么要归一化？</strong></p><p>直接计算的相似度可能数值范围差异很大（例如某些物品对的相似度为 100，另一些为 0.1），导致推荐结果偏向少数高分物品。将相似度缩放到统一范围（如 [0, 1]），避免数值量纲影响推荐排序。</p><p><strong>步骤4：生成推荐</strong></p><p><strong>找出候选物品</strong>：遍历用户历史物品$I_u$，找到每个物品Top-K个相似物品，合并去重后得到候选池C</p><p>例如：</p><ul><li>$A$ 的相似物品：${B, D, E}$（相似度分别为 $0.8,\ 0.6,\ 0.5$）</li><li>$B$ 的相似物品：${A, C, F}$（相似度分别为 $0.8,\ 0.7,\ 0.4$）</li></ul><table><thead><tr><th>步骤</th><th>说明</th></tr></thead><tbody><tr><td>用户历史交互</td><td>$I_u &#x3D; {A, B}$（用户已与 A、B 交互）</td></tr><tr><td>A 的相似物品</td><td>${B, D, E} \rightarrow$ 剔除 ${A, B}$ 后保留 ${D, E}$</td></tr><tr><td>B 的相似物品</td><td>${A, C, F} \rightarrow$ 剔除 ${A, B}$ 后保留 ${C, F}$</td></tr><tr><td>合并候选选池</td><td>$C &#x3D; {C, D, E, F}$（已去重）</td></tr></tbody></table><p><strong>计算推荐分数</strong>：对候选池中的每个物品<code>j∈C</code>，计算其与用户历史物品的加权相似度</p><p>$$<br>\text{Score}(j) &#x3D; \sum_{i \in I_u} \text{Sim}_{\text{norm}}(i, j)<br>$$</p><p>基于用户历史物品$I_u &#x3D; {A, B}$，计算物品D的分数：</p><p>$$<br>\text{Score}(D) &#x3D; \text{Sim}(A, D) + \text{Sim}(B, D) &#x3D; 0.6 + 0 &#x3D; 0.6<br>$$</p><p><strong>排序并生成推荐</strong>：按$\text{Score}(j)$降序排列候选物品，取Top-N作为推荐结果</p><table><thead><tr><th>候选物品</th><th>分数</th></tr></thead><tbody><tr><td>$C$</td><td>0.7</td></tr><tr><td>$D$</td><td>0.6</td></tr><tr><td>$E$</td><td>0.5</td></tr><tr><td>$F$</td><td>0.4</td></tr></tbody></table><p><strong>Top-3 推荐结果</strong>：$[C, D, E]$</p><h3 id="时间复杂度分析"><a href="#时间复杂度分析" class="headerlink" title="时间复杂度分析"></a>时间复杂度分析</h3><p><strong>Swing 算法的时间复杂度是多少？如何优化其在大规模数据下的性能？</strong></p><p><strong>时间复杂度</strong>：$O(n^2m^2)$（$n$ 为物品数，$m$ 为平均用户交互数），最坏情况下需遍历所有物品对和用户对。</p><p><strong>优化方法</strong></p><ul><li>剪枝：仅计算共现用户数超过阈值的物品对。</li><li>分片：按物品类别分片并行计算。</li><li>近似：使用局部敏感哈希（LSH）快速查找相似用户对。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;传统ItemCF的局限性&quot;&gt;&lt;a href=&quot;#传统ItemCF的局限性&quot; class=&quot;headerlink&quot; title=&quot;传统ItemCF的局限性&quot;&gt;&lt;/a&gt;传统ItemCF的局限性&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;热门物品偏差&lt;/strong&gt;：热门物品容易</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="Swing" scheme="http://example.com/tags/Swing/"/>
    
    <category term="热门商品误判成相似" scheme="http://example.com/tags/%E7%83%AD%E9%97%A8%E5%95%86%E5%93%81%E8%AF%AF%E5%88%A4%E6%88%90%E7%9B%B8%E4%BC%BC/"/>
    
    <category term="归一化" scheme="http://example.com/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Item-CF</title>
    <link href="http://example.com/2025/07/03/Item-CF/"/>
    <id>http://example.com/2025/07/03/Item-CF/</id>
    <published>2025-07-03T07:39:55.000Z</published>
    <updated>2025-07-05T08:37:22.164Z</updated>
    
    <content type="html"><![CDATA[<h3 id="对ItemCF算法的整体理解"><a href="#对ItemCF算法的整体理解" class="headerlink" title="对ItemCF算法的整体理解"></a>对ItemCF算法的整体理解</h3><p><strong>请简述ItemCF的核心思想及其主要步骤</strong></p><p><strong>核心思想</strong>：当很多用户同时喜欢两个物品时，认为这两个物品具有较高的相似度。</p><p><strong>计算物品相似度</strong>：基于用户-物品交互记录（如点击、评分等）计算所有物品之间的相似度。</p><p><strong>召回阶段逻辑</strong></p><ul><li>找到用户交互过的物品集合<code>I_u</code></li><li>对于每个<code>i∈I_u</code>，找与之相似的物品</li><li>将这些相似物品合并去重、排序后推荐给用户</li></ul><p><strong>对候选物品进行打分&#x2F;排序的预测</strong></p><p>用户 $u$ 对物品 $b$ 的预测评分，也就是$\hat{r}_{u,b}$等于</p><p>$$<br>\frac{\sum_{i \in I_u} \text{sim}(b, i) \cdot r_{u,i}}{\sum_{i \in I_u} |\text{sim}(b, i)|}<br>$$</p><p>其中：</p><ul><li>$I_u$：用户 $u$ 的历史交互物品集合  </li><li>$r_{u,i}$：用户 $u$ 对物品 $i$ 的评分或隐式反馈得分  </li><li>$\text{sim}(b, i)$：物品 $b$ 与用户交互物品 $i$ 的相似度<br>这是在召回之后，用来排序候选物品的重要打分函数。</li></ul><h3 id="相似度计算方法对比"><a href="#相似度计算方法对比" class="headerlink" title="相似度计算方法对比"></a>相似度计算方法对比</h3><p><strong>ItemCF中常用的相似度计算方法有哪些？分别适用于什么场景？</strong></p><table><thead><tr><th>方法</th><th>公式&#x2F;描述</th><th>适用场景</th></tr></thead><tbody><tr><td>离散型余弦相似度</td><td>向量为 0&#x2F;1（是否交互），计算夹角余弦</td><td>隐式反馈（如点击、浏览）</td></tr><tr><td>连续型余弦相似度</td><td>向量为权重（评分、时长），计算夹角余弦</td><td>显式反馈（如评分）、带权重的隐式反馈</td></tr><tr><td>欧式距离</td><td>计算向量几何距离，值越小越相似</td><td>需要幅度敏感的连续数据（如物理距离）</td></tr><tr><td>皮尔逊相关系数</td><td>协方差与标准差的归一化，衡量线性相关性</td><td>连续数据且需中心化（如评分）</td></tr><tr><td>杰卡德系数</td><td>交集大小除以并集大小，仅处理二值数据</td><td>无权重行为（如点赞、收藏）</td></tr></tbody></table><h3 id="余弦相似度的特点"><a href="#余弦相似度的特点" class="headerlink" title="余弦相似度的特点"></a>余弦相似度的特点</h3><p><strong>为什么余弦相似度在稀疏场景下更稳定有效？它如何解决用户评分标准不统一的问题？</strong></p><p><strong>稀疏场景适配性</strong>：只需两个物品有一个共同用户即可计算相似度，不依赖全局数据。</p><p><strong>评分标准统一</strong>：余弦相似度只关注向量方向（评分趋势），忽略长度（评分绝对值）。例如，用户A习惯打高分<code>(5/4)</code>，用户B习惯打低分<code>(3/2)</code>，但若两者对物品<code>(X/Y)</code>的打分趋势一致（X&gt;Y），则认为相似。</p><h3 id="皮尔逊相关系数的局限性"><a href="#皮尔逊相关系数的局限性" class="headerlink" title="皮尔逊相关系数的局限性"></a>皮尔逊相关系数的局限性</h3><p><strong>皮尔逊相关系数为什么不适用于稀疏场景？如何改进？</strong></p><p><strong>局限性</strong>：</p><ul><li>需要足够多的共同评分（否则均值估计不准，协方差计算不稳定）</li><li>要求数据具有中心化意义（如评分均值需有意义）</li></ul><p><strong>改进</strong>：填补缺失值（如用全局均值），或改用余弦相似度。</p><p><strong>既然皮尔逊相关系数在稀疏数据下有这么多限制，为什么实际中还有人用它？</strong></p><p><strong>纠正评分偏置</strong>：相比余弦相似度，皮尔逊会先对每个用户&#x2F;物品的评分做中心化处理（减去均值），这能有效消除评分风格的影响，比如某些用户习惯打高分&#x2F;低分，在相似度计算中体现更真实的兴趣相关性。</p><h3 id="混合相似度的设计"><a href="#混合相似度的设计" class="headerlink" title="混合相似度的设计"></a>混合相似度的设计</h3><p><strong>如何结合物品的类别信息提升ItemCF的相似度计算效果？</strong></p><p>在基础相似度（如余弦）上增加类别权重，例如：</p><p>若两物品属于同一类别，相似度乘以权重 $(1 + \alpha)$，其中$\alpha &gt; 0$</p><p>$$<br>\text{Sim}{\text{混合}} &#x3D; \text{Sim}{\text{基础}} \times (1 + \alpha \cdot \mathbb{I}[\text{同类别}])<br>$$</p><h3 id="杰卡德系数的应用"><a href="#杰卡德系数的应用" class="headerlink" title="杰卡德系数的应用"></a>杰卡德系数的应用</h3><p><strong>杰卡德系数适用于哪些行为数据？能否用于评分数据？</strong></p><p><strong>适用场景</strong>：二值行为（如是否点击、是否购买），不适用于连续数据（如评分）。</p><p><strong>评分数据处理</strong>：需先离散化（如评分≥4视为1，否则为0），但会损失信息。</p><h3 id="欧式距离与余弦相似度的区别"><a href="#欧式距离与余弦相似度的区别" class="headerlink" title="欧式距离与余弦相似度的区别"></a>欧式距离与余弦相似度的区别</h3><p><strong>欧式距离和余弦相似度在衡量物品相似度时有何本质区别？</strong></p><p><strong>欧式距离</strong>：关注向量绝对位置，对幅度敏感。适合需要区分“量级”的场景（如消费金额）。<br><strong>余弦相似度</strong>：关注向量方向，对幅度不敏感。适合忽略量级、关注趋势的场景（如评分偏好）。</p><h3 id="多行为数据的混合建模"><a href="#多行为数据的混合建模" class="headerlink" title="多行为数据的混合建模"></a>多行为数据的混合建模</h3><p><strong>在用户行为数据中，若80%的交互是点击，20%是购买，如何设计相似度计算？</strong></p><p><strong>加权混合</strong>：对不同行为赋予权重（如购买权重&#x3D;5，点击权重&#x3D;1），生成加权用户-物品矩阵，再用连续型余弦相似度计算。<br><strong>分层处理</strong>：分别计算点击和购买的相似度，加权融合（如 $\text{Sim} &#x3D; 0.2 \cdot \text{Sim}{\text{购买}} + 0.8 \cdot \text{Sim}{\text{点击}}$）。</p><h3 id="冷启动的常见解决方案"><a href="#冷启动的常见解决方案" class="headerlink" title="冷启动的常见解决方案"></a>冷启动的常见解决方案</h3><p><strong>ItemCF如何解决新物品的冷启动问题？请详细说明具体方法及其实现逻辑。</strong></p><p><strong>问题成因</strong>：物品冷启动，新物品缺乏交互数据导致无法计算相似度，也就无法与其他物品建立相似度关系</p><p><strong>解决</strong></p><p><strong>方法一：结合内容信息初始化相似度</strong></p><ul><li>提取物品内容特征：如类别、标签、描述文本（TF-IDF&#x2F;Embedding）、作者、价格等。</li><li>计算内容相似度：离散特征（类别&#x2F;标签）用杰卡德系数（共享标签比例）；连续特征（文本向量）用余弦相似度。</li><li>融合到ItemCF：新物品的相似度直接使用内容相似度，老物品的相似度保留原ItemCF结果。</li></ul><p>比如，新书《深度学习入门》与老书《机器学习实战》同属“AI”类别，且标签重合度高 → 初始化高相似度；当新书积累足够点击&#x2F;购买后，逐步过渡到行为数据计算的相似度。</p><p><strong>方法二：混合推荐策略</strong></p><p><strong>阶段1:冷启动期</strong></p><ul><li>对新物品采用基于内容（Content-Based）的推荐：用户画像 + 物品内容特征匹配（如用户常看“编程”类文章 → 推荐新书《Python进阶》）。</li><li>同时对新物品进行曝光试探：在推荐流中随机插入新物品，收集初始交互数据（需控制频率，避免用户体验下降）。</li></ul><p><strong>阶段2:数据累积后</strong></p><ul><li>设定切换阈值（如点击量≥100次或加入购物车≥10次）。</li><li>达到阈值后，将新物品纳入ItemCF相似度计算池，逐步替代内容推荐。</li></ul><p><strong>工程优化技巧</strong></p><ul><li>冷启动标识：在物品元数据中增加<code>is_cold_start</code>字段，动态更新状态。</li><li>AB测试：对比纯ItemCF与混合策略的点击率&#x2F;转化率，调整内容权重和切换阈值。</li><li>降级策略：若内容特征缺失，可基于热门推荐或类别相似度兜底。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;对ItemCF算法的整体理解&quot;&gt;&lt;a href=&quot;#对ItemCF算法的整体理解&quot; class=&quot;headerlink&quot; title=&quot;对ItemCF算法的整体理解&quot;&gt;&lt;/a&gt;对ItemCF算法的整体理解&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;请简述ItemCF的核心思想</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="协同过滤" scheme="http://example.com/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
    <category term="物品间的相似度计算" scheme="http://example.com/tags/%E7%89%A9%E5%93%81%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    
    <category term="冷启动" scheme="http://example.com/tags/%E5%86%B7%E5%90%AF%E5%8A%A8/"/>
    
  </entry>
  
</feed>
